{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd88ea27",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Exercise: Gaussian mixture models\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this exercise, we'll apply Gaussian Mixture Models to the Iris dataset to understand clustering dynamics, evaluate the effectiveness of different numbers of clusters, and explore the impact of covariance types on clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this exercise, you should be able to:\n",
    "- Confidently implement Gaussian Mixture Models using Scikit-learn on any dataset.\n",
    "- Evaluate model performance with different numbers of clusters.\n",
    "- Understand the effect of different covariance types on the clustering outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45a481",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f7d62",
   "metadata": {},
   "source": [
    "### Exercise 1: Load data\n",
    "For this exercise, we'll use the Iris dataset, a famous multivariate dataset introduced by Sir Ronald Fisher in 1936, as an example of discriminant analysis. The dataset consists of 150 samples from **three species of Iris flowers** (Iris setosa, Iris virginica, and Iris versicolor) with **four features**: the length and the width of the sepals and petals.\n",
    "\n",
    "1. Load the Iris dataset.\n",
    "   \n",
    "   **HINT**: Use the `load_iris()` function. \n",
    "2. Apply PCA to reduce the feature space from four dimensions to two. \n",
    "3. Plot the PCA-reduced data, colouring each point by its species label to visually assess the natural clusters within the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b7683",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2: Basic GMM application\n",
    "\n",
    "Apply a Gaussian Mixture Model to the Iris dataset.\n",
    "  1. Fit a GMM to the PCA-reduced data.\n",
    "  2. Use three components (as suggested by the original labels).\n",
    "  3. Plot the clustering results, showing the data points coloured by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b7dc6",
   "metadata": {},
   "source": [
    "### Exercise 3: Finding the optimal number of clusters\n",
    "\n",
    "Determine the optimal number of clusters for the Iris dataset using the Bayesian Information Criterion (BIC).\n",
    "\n",
    "  1. Fit GMMs with different numbers of components (from 1 to 6).\n",
    "  2. Calculate and plot the BIC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3200985",
   "metadata": {},
   "source": [
    "### Exercise 4: Effect of covariance type\n",
    "Analyse the impact of different covariance types on the clustering results.\n",
    "  1. Fit GMMs using the full, tied, diag, and spherical covariance types.\n",
    "  2. Plot the clustering results for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a339a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dea0d",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8f391",
   "metadata": {},
   "source": [
    "### Exercise 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338733e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Reduce dimensions for visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=150)\n",
    "plt.title(\"PCA of Iris Dataset\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdb88a",
   "metadata": {},
   "source": [
    "This preparation and visualisation step is crucial for understanding the data’s structure before applying clustering algorithms like Gaussian Mixture Models. It helps identify if the data clusters naturally and guides the choice of parameters for more complex clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10363b6",
   "metadata": {},
   "source": [
    "### Exercise 2: Basic GMM application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09386d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(X_pca)\n",
    "labels = gmm.predict(X_pca)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=40, cmap='viridis')\n",
    "plt.title(\"GMM Clustering of Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff325262",
   "metadata": {},
   "source": [
    "The plot illustrates the clustering results, where each colour represents one of the three clusters. The clusters appear distinct, indicating that the GMM has successfully identified underlying patterns in the data corresponding roughly to the Iris species. Each cluster's spread and separation suggest a good fit, which was our objective in selecting three components, mirroring the actual number of species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761920a",
   "metadata": {},
   "source": [
    "### Exercise 3: Finding the optimal number of clusters\n",
    "\n",
    "Since we already know we have three components in the Iris dataset, validating this assumption using a statistical method like the Bayesian Information Criterion (BIC) is always good practice. This approach reinforces our understanding of the dataset's structure and ensures that our model selection is justified based on the data rather than prior knowledge alone. The BIC helps determine the number of clusters by balancing model complexity and goodness of fit, providing a quantitative measure to support or challenge our assumptions. Testing different numbers of clusters can reveal whether three is indeed the optimal choice or if a more straightforward or more complex model could better capture the inherent patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34202109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIC for different number of components\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(X_pca)\n",
    "    bic.append(gmm.bic(X_pca))\n",
    "\n",
    "# Plot BIC scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_components_range, bic, marker='o')\n",
    "plt.title(\"BIC Scores for Different Numbers of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"BIC Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794085b6",
   "metadata": {},
   "source": [
    "The plot shows the Bayesian Information Criterion (BIC) scores for models with varying numbers of clusters, ranging from 1 to 6. \n",
    "\n",
    "The BIC scores decrease sharply as we move from one to two clusters, suggesting a significant improvement in model fit. The BIC score reaches its lowest point at two clusters, indicating that a model with two clusters best balances model complexity with fit to the data. As the number of clusters increases from two to six, the BIC scores level off and gradually increase, suggesting that adding more clusters beyond two does not provide a better-fit relative to the increase in complexity. Therefore, according to this BIC analysis, the optimal number of clusters is two.\n",
    "\n",
    "This observation, despite the known classification of the Iris dataset into three species, indicates potential model misfit or data overlap. This discrepancy can occur due to several reasons: \n",
    "- the inherent overlap between the species in the dataset.\n",
    "- PCA-induced data simplification that might have obscured some nuances between classes. \n",
    "- BIC's penalty for model complexity favours simpler models when the distinction between some classes is not stark. \n",
    " \n",
    "This scenario underscores the importance of combining statistical model selection criteria with domain knowledge and potentially other model validation techniques to ensure a comprehensive understanding and accurate classification in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02761ad8",
   "metadata": {},
   "source": [
    "### Exercise 4: Effect of covariance type\n",
    "\n",
    "The provided solution explores how different covariance types affect the clustering behaviour of Gaussian Mixture Models (GMM) applied to the PCA-reduced Iris dataset. The code fits GMMs using four covariance types—'full', 'tied', 'diag', and 'spherical'— each defining how the variance and covariance are structured across clusters:\n",
    "\n",
    "- **Full**: Each cluster has its own general covariance matrix, allowing for ellipsoidal shapes in any orientation.\n",
    "- **Tied**: All clusters share the same general covariance matrix, which can express complex shapes but is the same across clusters.\n",
    "- **Diag**: Each cluster has its own diagonal covariance matrix, restricting the cluster's shape to an axis-aligned ellipsoid.\n",
    "- **Spherical**: Each cluster must be spherical, with all dimensions having equal variance, simplifying the geometry of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2217ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMMs with different covariance types\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "for cov_type in covariance_types:\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)\n",
    "    gmm.fit(X_pca)\n",
    "    labels = gmm.predict(X_pca)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', edgecolor='k', s=150)\n",
    "    plt.title(f\"GMM with {cov_type} covariance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e517b",
   "metadata": {},
   "source": [
    "\n",
    "The resulting plots visually depict how these assumptions influence the clustering results. The 'full' covariance allows for the most flexible cluster shapes, adapting to the data's natural distribution. 'Tied' covariance maintains complexity but forces the same shape across all clusters. 'Diag' and 'spherical' increasingly restrict the cluster shapes, which can lead to a less accurate representation of data relationships if the actual structure is complex. Each plot shows clusters marked by different colours, illustrating the spatial distribution and separation achieved under each covariance assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
